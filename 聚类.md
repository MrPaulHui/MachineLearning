# ML中的聚类

## KMeans

参考：https://www.cnblogs.com/pinard/p/6164214.html

### 原理

设数据集为$\{x_i\},i=1,2,...,N$，聚成k类，为$C_1,C_2,...,C_k$，每一类的聚类中心为$\mu_i=\frac{1}{|C_i|}\sum_{x\in C_i}x$，$i=1,2,...,k$，即这个类所有样本向量的均值。优化目标是
$$
\min E=\sum_{i=1}^k\sum_{x\in C_i}||x-\mu_i||_2^2
$$
这是一个NP hard问题（需要再看），只可用启发式方法。

### 算法步骤

设启发式方法迭代次数为epoch

1. 从数据集中随机选择k个样本向量作为初始的聚类中心$\{\mu_1,\mu_2,...,\mu_k\}$
2. 对每一次迭代，进行如下操作：
   1. 每次迭代重新初始化k个聚类结果集合$C_j=\empty,j=1,2,...,k$
   2. 对每一个样本$x_i$，$i=1,2,...,N$，计算其与所有k个聚类中心的距离$d_{ij}=||x_i-\mu_j||_2^2$，$j=1,2,...,k$，求该样本对应的聚类标签$\lambda_i=\arg\min_jd_{ij}$，$\lambda_i\in\{1,2,...,k\}$，即在现在的聚类中心条件下，该样本应该被聚到和聚类中心距离最小的一类。更新对应的聚类结果集合$C_{\lambda_i}=C_{\lambda_i}\cup \{x_i\}$
   3. 根据更新后的聚类结果集合，重新计算聚类中心$\mu_j=\frac{1}{|C_j|}\sum_{x\in C_j}x$，$j=1,2,...,k$
   4. 若重新计算得到的聚类中心与上次迭代的没有变化，则结束循环，否则重复上面3个步骤；若迭代次数超过epoch，则结束循环。
3. 输出$\{C_1,C_2,...,C_k\}$

### 优点

1. 原理简单，实现简单，收敛速度快
2. 算法可解释性强
3. 超参数少，只需要设定聚类簇数

### 缺点和对应的解决措施

#### 1. k值的选择不好确定

解决措施：

1. **肘部法则**，取多个k值，绘制误差E（即优化目标）曲线，突然下降趋势和缓慢下降趋势的交汇点即为最佳的k值。
2. **ISODATA算法**，见《百面机器学习》

#### 2. 算法只能收敛到局部最优，受初始值影响大

解决措施：**KMeans++算法**，原始的KMeans算法随机选择k个初始聚类中心，KMeans++采用策略来选择初始聚类中心，策略为：假设已经选择了n个聚类中心（n<k），**那么选择第n+1个聚类中心时，选择距离当前n个中心最远的点**，第一个中心随机选择。从直观上解释，聚类中心离得越远越好。

#### 3. 易受噪声点和离群点的影响

#### 4. 样本只能被划分到单一的类别

### KMeans算法收敛性证明（高阶问题）

## GMM 高斯混合模型

### EM算法

#### 最大似然估计和最大后验估计

参考：

《统计学习方法》

《百面机器学习》

https://zhuanlan.zhihu.com/p/40024110

https://blog.csdn.net/u011508640/article/details/72815981

[贝叶斯估计]([http://noahsnail.com/2018/05/17/2018-05-17-%E8%B4%9D%E5%8F%B6%E6%96%AF%E4%BC%B0%E8%AE%A1%E3%80%81%E6%9C%80%E5%A4%A7%E4%BC%BC%E7%84%B6%E4%BC%B0%E8%AE%A1%E3%80%81%E6%9C%80%E5%A4%A7%E5%90%8E%E9%AA%8C%E6%A6%82%E7%8E%87%E4%BC%B0%E8%AE%A1/](http://noahsnail.com/2018/05/17/2018-05-17-贝叶斯估计、最大似然估计、最大后验概率估计/))

https://zhuanlan.zhihu.com/p/36331115

前提是数据集独立同分布，这也是ML的重要前提。

最大似然估计就是求模型参数，使得通过模型输出的分布和观测到的分布尽可能一致。

最大后验估计相当于给最大似然估计加上一个参数分布的先验，防止其因为数据量少等原因过于走偏。也可以理解为给最大似然加上一个正则项约束。二者本质上都是求给定数据样本和模型，求最适合的模型参数。（其实就是机器学习啊）

贝叶斯估计在最大后验估计基础上，进一步优化了先验的表示。

##### 公式表示

设Y为观测的随机变量数据，$\theta$为需要估计的模型参数

则Y的似然函数为$P(Y|\theta)$，对数似然函数为$L(\theta)=\log P(Y|\theta)$

极大似然估计优化目标：
$$
\max_\theta P(Y|\theta)
$$
等价于优化对数似然函数，
$$
\max_\theta L(\theta)=\max_\theta \log P(Y|\theta)
$$
最大后验估计优化目标：
$$
\max_\theta P(\theta|Y)=\max_\theta\frac{P(Y|\theta)P(\theta)}{P(Y)}
$$
又$P(Y)$是确定的值，所以等价于
$$
\max_\theta P(Y|\theta)P(\theta)
$$

#### EM算法推导

若**模型中存在隐变量**，则无法直接使用最大似然估计或最大后验估计，（具体情境参考《统计学习方法》例9.1），这时候就需要使用EM算法。**EM算法就是含有隐变量的极大似然估计或含有隐变量的极大后验估计。**

设观测变量为Y，隐变量为Z，模型参数为$\theta$，

以含有隐变量的极大似然估计为例，优化目标为
$$
\max_\theta L(\theta)=\max_\theta \log P(Y|\theta)=\max_\theta \log\sum_ZP(Y,Z|\theta)=\max_\theta\log\sum_Z[P(Y|Z,\theta)P(Z|\theta)]
$$
一般极大似然估计都是采用取log后，写成log项相加，再求导，但这个log里面有相加，求导困难，所以采用启发式方法迭代。

目标是最大化$L(\theta)$，所以每次迭代，都要使$L(\theta)$增大，设第i次迭代后，得到的参数估计值是$\theta^{(i)}$，那么对于下一次迭代的新估计值$\theta$，应该有$L(\theta)-L(\theta^{(i)})>0$，且差值越大越好。
$$
L(\theta)-L(\theta^{(i)})=\log(\sum_ZP(Y|Z,\theta)P(Z|\theta))-\log P(Y|\theta^{(i)})
$$
根据Jensen不等式$\log\sum_j\lambda_j y_j\geq\sum_j\lambda_j\log y_j$，其中$\lambda_j\geq0,\sum_j\lambda_j=1$，可以得到差值的下界
$$
L(\theta)-L(\theta^{(i)})=\log(\sum_ZP(Y|Z,\theta)P(Z|\theta))-\log P(Y|\theta^{(i)})\\
=\log(\sum_Z P(Z|Y,\theta^{(i)})\frac{P(Y|Z,\theta)P(Z|\theta)}{P(Z|Y,\theta^{(i)})})-\log P(Y|\theta^{(i)})\\
\geq\sum_ZP(Z|Y,\theta^{(i)})\log\frac{P(Y|Z,\theta)P(Z|\theta)}{P(Z|Y,\theta^{(i)})}-\log P(Y|\theta^{(i)})\\
\geq\sum_Z[P(Z|Y,\theta^{(i)})\log\frac{P(Y|Z,\theta)P(Z|\theta)}{P(Z|Y,\theta^{(i)})}-\log P(Y|\theta^{(i)})]\\
=\sum_ZP(Z|Y,\theta^{(i)})\log\frac{P(Y|Z,\theta)P(Z|\theta)}{P(Z|Y,\theta^{(i)})P(Y|\theta^{(i)})}
$$
令
$$
B(\theta,\theta^{(i)})=L(\theta^{(i)})+\sum_ZP(Z|Y,\theta^{(i)})\log\frac{P(Y|Z,\theta)P(Z|\theta)}{P(Z|Y,\theta^{(i)})P(Y|\theta^{(i)})}
$$
则有
$$
L(\theta)\geq B(\theta,\theta^{(i)})
$$
所以$B(\theta,\theta^{(i)})$为$L(\theta)$的一个下界，当$\theta=\theta^{(i)}$时取等。

所以能让$B(\theta,\theta^{(i)})$增大的$\theta$，也可以使$L(\theta)$增大，为了使$L(\theta)$尽可能的大，应该使其下界$B(\theta,\theta^{(i)})$达到极大，所以
$$
\theta^{(i+1)}=\arg \min_\theta B(\theta,\theta^{(i)})
$$
忽略对$\theta$而言是常数的项，有
$$
\theta^{(i+1)}=\arg \min_\theta B(\theta,\theta^{(i)})\\
=\arg \min_\theta[\sum_ZP(Z|Y,\theta^{(i)})\log{P(Y|Z,\theta)P(Z|\theta)}]\\
=\arg \min_\theta[\sum_ZP(Z|Y,\theta^{(i)})\log{P(Y,Z|\theta)}]
$$
记
$$
Q(\theta,\theta^{(i)})=\sum_ZP(Z|Y,\theta^{(i)})\log{P(Y,Z|\theta)}
$$
所以
$$
\theta^{(i+1)}=\arg \min_\theta Q(\theta,\theta^{(i)})
$$

#### EM算法步骤整理

1. 设定参数初始值$\theta^{(0)}$

2. E步：计算期望Q
   $$
   Q(\theta,\theta^{(i)})=\sum_ZP(Z|Y,\theta^{(i)})\log{P(Y,Z|\theta)}
   $$

3. M步：计算使期望最大的新参数
   $$
   \theta^{(i+1)}=\arg \min_\theta Q(\theta,\theta^{(i)})
   $$

4. 重复第2,3步，直到收敛

#### EM算法收敛性证明（高阶问题）

### 高斯混合模型

#### 核心思想

两个假设：

1. 假设数据集样本从从多个高斯分布中生成
2. 假设同一类样本符合相同的高斯分布

设数据集由k个标准高斯分布生成，每个高斯分布的参数为：均值$\mu_j$，标准差$\sigma_j$，所占权重（使用这个模型来生成数据的概率）$\alpha_j$，$j=1,2,...,k$。高斯混合模型可以表示为
$$
P(y|\theta)=\sum_{j=1}^k\alpha_j\phi(y|\mu_j,\sigma_j)
$$
其中，$\phi$为高斯分布概率密度
$$
\phi(y|\mu_j,\sigma_j)=\frac{1}{\sqrt{2\pi}\sigma_j}\exp(-\frac{(y-\mu_j)^2}{2\sigma_j^2})
$$
模型参数$\theta=\{\alpha_1,...,\alpha_k;\mu_1,...,\mu_k;\sigma_1,...,\sigma_k\}$

设第j个高斯分布对应的聚类簇为$C_j$，$j=1,2,... ,k$

求解出$P(y|\theta)$后，计算每个样本属于各个聚类簇的概率
$$
P(y_i\in C_j|y_i)=\frac{P(y_i\in C_j,y_i)}{P(y_i)}=\frac{P(y_i|y_i\in C_j)P(y_i\in C_j)}{P(y_i)}
$$
其中，
$$
P(y_i|y_i\in C_j)=\phi(y_i|\mu_j,\sigma_j)\\
P(y_i\in C_j)=\alpha_j
$$
所以，
$$
P(y_i\in C_j|y_i)=\frac{\alpha_j\phi(y_i|\mu_j,\sigma_j)}{\sum_{j=1}^k\alpha_j\phi(y|\mu_j,\sigma_j)}
$$
取概率最大的聚类簇作为该样本的聚类结果，并且**可以得到属于该聚类簇的概率**。

#### 使用EM算法求解

留个坑待填

### 相比KMeans的优点

1. 可以得到样本属于某类的概率
2. 可以用于概率密度的估计
3. 可以用于生成新数据样本

## 谱聚类

留个坑待填

## 聚类算法评价指标

留个坑待填