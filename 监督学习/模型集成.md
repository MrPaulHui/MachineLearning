# model ensemble

参考：

https://www.jiqizhixin.com/articles/2019-05-15-15

https://www.cnblogs.com/liuwu265/p/4690486.html

面试问题总结：https://blog.csdn.net/qq_34896915/article/details/73771287

## ML中的偏差和方差

参考：

https://blog.csdn.net/u010626937/article/details/74435109

https://zhuanlan.zhihu.com/p/86263786

**偏差bias：针对一个模型，模型输出与真实label的差距**（训练集数据）

**方差variance：针对多个模型，表示多个模型的差异程度**。（多个模型是指用同一个训练集，多次训练得到的不同的模型）。

越简单的模型，对训练集数据的拟合能力越差，导致高偏差，但是每次训练都会得到差不多的模型（因为模型本身很简单），即低方差。这种情况对应的是欠拟合。

越复杂的模型，对训练集数据的拟合能力越强，即低偏差，但是每次训练都会得到差异很大的模型（因为模型复杂，稍微的变化就带来模型很大的变化），导致高方差。这种情况对应过拟合。

（个人理解，**偏差-方差和欠拟合-过拟合不是因果关系**，即不是因为高方差导致了过拟合，二者是一个并列关系，从不同的角度描述了同一个拟合现象——复杂的模型导致高方差，同时复杂的模型会过分拟合训练集数据，导致一些不是本质的特征被过分学习了，进而导致对测试数据效果不好，泛化能力不强，即过拟合了。）

总结一下，

**简单模型——高偏差低方差——欠拟合**

**复杂模型——低偏差高方差——过拟合**

## 基础概述

同质学习器：使用同一种模型训练出的学习器

异质学习器：使用不同种模型训练出的学习器

弱学习器定义为每个基础的学习器，ensemble之后形成强学习器。

ensemble方法大概包括三种：

- bagging：针对同质学习器；弱学习器相互独立的学习；按照某种确定性的平均过程将它们组合起来；目的是降低方差，提高模型鲁棒性，即减小过拟合
- boosting：针对同质学习器；弱学习器顺序进行学习，每个弱学习器都依赖前面的弱学习器；按照某种确定性的策略将它们组合起来；目的是降低偏差，提高模型的准确性
- stacking：针对异质学习器；弱学习器相互独立的学习；通过训练一个「元模型」将它们组合起来；目的是降低偏差，提高模型的准确性

ensemble集成学习主要考虑两个点：

1. 弱学习器的**拟合方式**（单个模型的训练方法，以及**单个模型训练的训练集抽样方法**）
2. 弱学习器的**聚合方式**（**多个模型怎么组合**，怎么加权）

## boosting

参考：《统计学习方法》

<img src="https://images2015.cnblogs.com/blog/1042406/201612/1042406-20161204194331365-2142863547.png" alt="img" style="zoom:50%;" />

boost本意就是提升，所以**针对的是提升模型的性能**，也就是准确性。从方差-偏差角度说，就是**降低偏差**，所以**boosting的基础弱学习器应该是高偏差低方差的模型，也就是简单模型**。

### 前向分步算法

model ensemble最基本的就是用加法模型把各个基础的弱学习器组合起来
$$
f(x)=\sum_{m=1}^M\beta_mb(x;\gamma_m)
$$
其中，$b(x;\gamma_m)$为弱学习器，$\gamma_m$为弱学习器的参数，$\beta_m$为弱学习器的所占权重系数。

直接对这个整体的模型训练是很困难的，优化的方法是前向分步算法，从前往后每一步只训练一个弱学习器，具体做法为：

设训练集$T=\{(x_1,y_1),(x_2,y_2),...,(x_N,y_N)\}$

在第m步，已经得到了之前m-1步学习到的“部分强学习器”$f_{m-1}(x)$，前m步的“部分强学习器”为
$$
f_m(x)=f_{m-1}(x)+\beta_mb(x;\gamma_m)
$$
学习第m个弱学习器的方法为
$$
\beta_m,\gamma_m=\arg\min_{\beta,\gamma}\sum_{i=1}^NL(y_i,f_{m-1}(x_i)+\beta b(x_i;\gamma))
$$
其中L为损失函数。目的是组合了第m个弱学习器的整体强学习器loss更低。

**个人理解，前向分步算法就是boosting的核心灵魂**。至于AdaBoost算法，就是损失函数具体化指数损失函数推导出的具体算法。同理，如果损失函数具体化为MSE，就成为了L2Boost算法（统计学习方法中提升回归树实际上就是L2Boost）。**而对任意损失函数的回归问题（弱学习器为回归模型），用梯度提升Gradient Boost是通用的解法。**

### AdaBoost

参考：

https://zhuanlan.zhihu.com/p/39972832

拟合方式上，每个弱学习器用各自的模型训练方法对训练集训练得到，但是每次迭代时，对上次迭代得到的弱学习器拟合错误的样本施加更大的权重。

聚合方式上，每个弱学习器的误差作为设定权重的indicator，误差越大的弱学习器，给其赋予的权重越小。

理论上任何弱学习器都可以用于Adaboost。但一般来说，使用最广泛的Adaboost弱学习器是决策树和神经网络。对于决策树，Adaboost分类用了CART分类树，而Adaboost回归用了CART回归树。

#### 前向分步算法推导AdaBoost二分类问题

设训练集$T=\{(x_1,y_1),(x_2,y_2),...,(x_N,y_N)\}$，其中$x_i\in R^d$，$y_i\in \{-1,1\}$，为二分类问题

最终分类器为
$$
G(x)=\sum_{m=1}^M\alpha_mG_m(x)
$$
其中，$G_m(x)$为第m个弱分类器，共有M个这样的弱分类器，$\alpha_m$为第m个弱分类器对应的权重。

可见，AdaBoost二分类是一个典型的加法模型，所以可用前向分步算法求解。

首先，需要明确损失函数为**指数损失函数**
$$
L(y,f(x))=\exp(-yf(x))
$$
在第m步，已经得到了之前m-1步学习到的“部分强学习器”$f_{m-1}(x)$，学习第m个弱学习器为
$$
\alpha_m,G_m=\arg\min_{\alpha,G}\sum_{i=1}^NL(y_i,f_{m-1}(x)+\alpha G(x_i))\\
=\arg\min_{\alpha,G}\sum_{i=1}^N\exp\{-y_i[f_{m-1}(x_i)+\alpha G(x_i)]\}\\
=\arg\min_{\alpha,G}\sum_{i=1}^N\hat w_{mi}\exp(-\alpha y_iG(x_i))
$$
其中，$\hat w_{mi}=\exp[-y_if_{m-1}(x_i)]$是一个和最小化无关的系数

对优化问题进一步整理，有
$$
\sum_{i=1}^N\hat w_{mi}\exp(-\alpha y_iG(x_i))=\sum_{y_i=G(x_i)}\hat w_{mi}e^{-\alpha}+\sum_{y_i\neq G(x_i)}\hat w_{mi}e^{\alpha}\\
=(e^{\alpha}-e^{-\alpha})\sum_{i=1}^N\hat w_{mi}I(y_i\neq G(x_i))+e^{-\alpha}\sum_{i=1}^N\hat w_{mi}
$$
对$\alpha$求导，并令导数为0，得到
$$
\alpha_m=\frac{1}{2}\log\frac{1-e_m}{e_m}
$$
其中，$e_m$为误差率
$$
e_m=\frac{\sum_{i=1}^N\hat w_{mi}I(y_i\neq G(x_i))}{\sum_{i=1}^N\hat w_{mi}}\\
=\sum_{i=1}^Nw_{mi}I(y_i\neq G(x_i))
$$

$$
w_{mi}=\frac{\hat w_{mi}}{\sum_{i=1}^N\hat w_{mi}}
$$

又$\hat w_{mi}=\exp[-y_if_{m-1}(x_i)]$，所以可得到下一次迭代的$\hat w_{m+1,i}$和$w_{m+1,i}$
$$
\hat w_{m+1,i}=\exp[-y_if_m(x_i)]=\exp[-y_i(f_{m-1}(x_i)+\alpha_mG_m(x_i))]\\
=\hat w_{mi}\exp(-\alpha_my_iG_m(x_i))
$$

$$
w_{m+1,i}=\frac{\hat w_{m+1,i}}{\sum_{i=1}^N\hat w_{m+1,i}}
$$

#### AdaBoost二分类算法整理

算法迭代步骤：

1. 初始化训练数据权值分布
   $$
   D_1=(w_{1,1},w_{1,2},...,w_{1,N}),\ \ w_{1,i}=\frac{1}{N}
   $$

2. 对$m=1,2,...,M$：

   1. 使用具有权值分布$D_m$的训练集，训练得到弱分类器$G_m(x)$

   2. 计算弱分类器$G_m(x)$在训练集上的分类误差
      $$
      e_m=\sum_{i=1}^Nw_{mi}I(G_m(x_i)\neq y_i)
      $$

   3. 计算弱分类器$G_m(x)$的系数
      $$
      \alpha_m=\frac{1}{2}\log\frac{1-e_m}{e_m}
      $$
      可以发现$e_m$越大，$\alpha_m$越小

   4. 更新训练集的权重分布
      $$
      D_{m+1}=(w_{m+1,1},w_{m+1,2},...,w_{m+1,N})
      $$

      $$
      w_{m+1,i}=w_{m,i}\frac{\exp(-\alpha_my_iG_m(x_i))}{\sum_{i=1}^Nw_{m,i}\exp(-\alpha_my_iG_m(x_i))}
      $$

      可以发现，对二分类问题如果标签和输出不一致，则乘机为负，指数项为正，权重就会增大，除分母的目的是进行归一化，使得权重和为1

算法终止条件为误差为0或小于阈值。

#### 多分类情况

上面的算法是针对二分类问题，多分类原理类似，只需在两个地方进行修改

2.3步中，弱分类器$G_m (x)$的系数修改为
$$
\alpha_m=\frac{1}{2}\log\frac{1-e_m}{e_m}+\log(R-1)
$$
2.4步中，更新权重的公式修改为
$$
w_{m+1,i}=\begin{cases}w_{m,i}\frac{\exp(-\alpha_m)}{Z_m}& y_i=G_m(x_i)\\
w_{m,i}\frac{\exp(\alpha_m)}{Z_m}& y_i\neq G_m(x_i)\end{cases}
$$
其中，$Z_m$为归一化因子
$$
Z_m=\sum_{i=1}^Nw_{m,i}\exp(-\alpha_mI_1(y_i=G_m(x_i)))
$$

$$
I_1(y_i=G_m(x_i))=\begin{cases}1& y_i=G_m(x_i)\\
-1& y_i\neq G_m(x_i)\end{cases}
$$

#### 提升树——分类决策树+AdaBoost

参考：https://juejin.im/post/5dd8b0a6f265da7dde7687c5

将弱分类器具体化为决策树，就成为提升树模型。

由于boost要求弱分类器为简单模型，所以对单个决策树而言，要有尽可能低的复杂度，所以**每个基础决策树只有一个根节点和两个叶节点**，**即最简单的CART分类决策树**。

以决策树为弱分类器用于一般的AdaBoost分类算法，即可得到提升树模型，不过有个细节问题——决策树的生成过程根据数据集进行特征选择，那怎么做到根据上次迭代的弱分类器的分类结果来训练这次的呢？数据集的权重分布又怎么体现呢？

解决方法是当前迭代中，**对数据集进行重新可重复抽样，当前数据集中权重越大的样本被抽到的概率越大，被抽到的次数就会越多，从而在抽样后的数据集中有更大的影响（实际上，这也是boosting最基本的根据权重抽样思路）**。用这种方法体现了重点训练上次分类错的样本的要求。

#### AdaBoost回归

**AdaBoost一般多用于分类**，回归一般用Gradient Boost比较多。用于回归的AdaBoost算法一般为AdaBoost.R2，参照：https://zhuanlan.zhihu.com/p/39972832，有空再补或者不看了。

用决策树作为AdaBoost回归的弱学习器，一般用CART回归树，且一般是只有根节点和两个叶节点的最简单CART树。

#### AdaBoost控制过拟合

**在每个弱学习器前面加一个小于1的参数。**

原理：每次走一小步逐渐逼近结果的效果，要比每次迈一大步很快逼近结果的方式更容易避免过拟合。即它不完全信任每一个弱学习器，它认为每个弱学习器只学到了真理的一小部分，累加的时候只累加一小部分，通过多学几个弱学习器弥补不足。
$$
G(x)=\sum_{m=1}^Mv\alpha_mG_m(x)
$$
其中，$0<v\le 1$，为正则化参数。

下面的Gradient Boost也是采取这种方式进行抑制过拟合。

#### AdaBoost优缺点

- 优点
  1. 作为分类器，精度很高
  2. 可以用任意的分类或回归模型作为弱学习器，灵活易用
  3. 不容易发生过拟合
- 缺点
  1. 对异常样本敏感，异常样本在迭代中可能会获得较高的权重，影响最终的强学习器的预测准确性

### Gradient Boost

GB一般用于回归问题。（也可用于基础回归的分类问题，比如sigmoid，softmax，实际上神经网络也是这种分类模式。纯分类问题应该就是贝叶斯模型，分类回归树这种，这一块可以单独整理一下。）

还是基于加法模型和前向分步算法，不过每个弱学习器的系数为1（？存疑）
$$
f(x)=\sum_{m=1}^Mb(x)
$$
所以，在第m步，求第m个弱学习器为
$$
b_m=\arg\min_{b}\sum_{i=1}^NL(y_i,f_{m-1}(x_i)+b(x_i))
$$
考虑前m-1步的“部分强学习器”$f_{m-1}(x)$，其loss为$L(y,f_{m-1}(x))$，将$f_{m-1}(x)$视作自变量，第m个弱学习器$b_m$要加到这个自变量上，且加上去之后应该使这个loss进一步减小，那么这个自变量加上一个什么值会使loss函数值变小呢？当然是loss值关于自变量$f_{m-1}(x)$的梯度值的相反数，也就是减去梯度值，就是梯度下降的原理。

所以第m个弱学习器应该去拟合loss值关于自变量$f_{m-1}(x)$的梯度值的相反数，即
$$
r_{mi}=-\frac{\part L(y_i,f_{m-1}(x_i))}{\part f_{m-1}(x_i)}
$$
让弱学习器$b_m(x)$去拟合$r_{mi}$

#### GBDT

Gradient Boost Decision Tree，用决策树作为具体弱学习器模型的Gradient Boost，且规定决策树为CART回归树，并且boost要求弱学习器为简单模型，所以CART回归树的深度一般不超过5层。

GBDT每次都使用全部数据训练，不抽样（当然，具体实现的时候会采取抽样防止过拟合）

GBDT具体算法流程：

输入：设训练集$T=\{(x_1,y_1),(x_2,y_2),...,(x_N,y_N)\}$，其中$x_i\in R^d$，$y_i\in R$，损失函数$L(y,f(x))$

输出：回归树$f(x)$

算法：

1. 初始化$f_0(x)$
   $$
   f_0(x)=\arg\min\sum_{i=1}^NL(y_i,c)
   $$
   可以发现，这是一个树桩，只有一个叶节点就是根节点

2. 对m=1,2,...,M：

   1. 计算
      $$
      r_{mi}=-\frac{\part L(y_i,f_{m-1}(x_i))}{\part f_{m-1}(x_i)}
      $$
      其中，$i=1,2,...,N$

   2. 对$r_m$拟合一个回归树$b_m (x)$，得到第m个回归树$b_m (x)$的叶节点区域$R_{mj}$，$j=1,2,...,J$

   3. 对每个叶节点区域，计算
      $$
      c_{mj}=\arg\min_{c}\sum_{x_i\in R_{mj}}L(y_i,f_{m-1}(x_i)+c)
      $$
      注意，一般回归树生成后，就自带了各个叶节点区域对应的输出值，那么为什么还要有这第3步呢？我想应该是：一般的回归树在生成过程中采用的是最小均分误差策略选择最优的划分特征和划分点，并由此得到每个区域对应的输出值为区域样本label的均值。在这里，第2步同样采用最小均分误差策略得到区域划分，但**如果直接用最小均分误差得到的输出值，就可能不符合L这个损失函数的要求，因为L不一定是L2 loss，所以就对每个区域，重新计算最符合设定的loss的输出值**。

   4. 更新$f_m(x)=f_{m-1}(x)+\sum_{j=1}^Jc_{mj}I(x\in R_{mj})$

3. 最终GBDT模型为
   $$
   f(x)=f_M(x)=\sum_{m=1}^M\sum_{j=1}^Jc_{mj}I(x\in R_{mj})
   $$

##### GBDT防止过拟合

1. 和AdaBoost一致，在每个树模型前加一个正则化参数，即
   $$
   f_m(x)=f_{m-1}(x)+vb(x)
   $$
   其中，$0<v\le 1$，为正则化参数。

   参考：https://blog.csdn.net/shine19930820/article/details/65633436

2. 从数据集中随机抽样。原理：降低基模型之间的相关性（再看）。

#### XGBoost

参考：https://www.jianshu.com/p/b0e147de75e6

XGBoost是对GBDT的进一步改进，其中Gradient Boost的思想是一致的，改进在于以下几个方面：

1. GBDT用CART作为基模型，而xgboost还支持线性模型（一般还是用决策树作为基
2. GBDT训练当前模型时，用到的是上一次迭代模型的一阶导数信息，而XGBoost使用了一阶和二阶导数信息。这也是对一般Gradient Boost算法的改进。
3. XGBoost在损失函数中加入了正则化项，控制过拟合
4. 借鉴随机森林中的列抽样，即随机特征选取，进一步控制过拟合

重点是第2点的一阶二阶导数信息和第3点的损失函数正则化

下面的符号定义沿用上面的。

##### 损失函数加入正则项

设用于分类或回归的损失函数为$l(y,f(x))$，定义正则项为
$$
\sum_{m=1}^M\Omega(b_m)
$$
其中，$b_m$为每一个基础模型。

整体损失函数为
$$
L(y,f(x))=l(y,f(x))+\sum_{m=1}^M\Omega(b_m)
$$
对于基础模型为决策树的情况，决策树的过拟合程度受**叶节点个数和叶节点输出值**的影响，两个都是越小越能抑制过拟合，所以定义具体的正则化项为
$$
\Omega(b_m)=\gamma T_m+\frac{1}{2}\lambda ||c_m||_2^2
$$
其中，$T_m$为第m个决策树的叶节点个数，$c_m$为第m个决策树的叶节点输出值向量。$\gamma,\lambda$为两个正则化参数。

##### 二阶泰勒展开求解当前树模型

前向分步算法的核心是在第m步迭代时，学习第m个基础模型
$$
b_m=\arg\min_{b}\sum_{i=1}^NL(y_i,f_{m-1}(x_i)+b(x_i))
$$
如何求解这个问题，GBDT采用的是利用一阶梯度的梯度下降法，即将$f_{m-1}(x_i)$视作自变量，求其一阶梯度，用负梯度作为label来训练$b_m$。

对于XGBoost，首先，损失函数加入了正则项，所以
$$
b_m=\arg\min_b\sum_{i=1}^Nl(y_i,f_{m-1}(x_i)+b(x_i))+\Omega(b)
$$
对损失函数表达式中的分类或回归损失$l$做二阶泰勒展开（注意这里的自变量是$f_{m-1}(x_i)+b(x_i)$），在$f_{m-1}(x_i)$处展开
$$
l(y_i,f_{m-1}(x_i)+b(x_i))=l(y_i,f_{m-1}(x_i))+g_ib(x_i)+\frac{1}{2}h_ib^2(x_i)
$$
其中，$g_i$为一阶导数
$$
g_i=\frac{\part l(y_i,f_{m-1}(x_i))}{\part f_{m-1}(x_i)}
$$
$h_i$为二阶导数
$$
h_i=\frac{\part^2 l(y_i,f_{m-1}(x_i))}{\part f_{m-1}(x_i)}
$$
$g_i$和$h_i$都为确定值。

所以第m步的整体loss可以写为
$$
L_m=\sum_{i=1}^N[l(y_i,f_{m-1}(x_i))+g_ib(x_i)+\frac{1}{2}h_ib^2(x_i)]+\Omega(b)
$$
去除常数项后，为
$$
L_m=\sum_{i=1}^N[g_ib(x_i)+\frac{1}{2}h_ib^2(x_i)]+\Omega(b)
$$
将基础模型b具体化为决策树，该决策树有$T_m$个叶节点，即$T_m$个区域，每个区域的输出值为$c_{mj}$，则
$$
b(x)=\sum_{j=1}^{T_m}c_{mj}I(x\in R_j)
$$

$$
\Omega(b)=\gamma T_m+\frac{1}{2}\lambda \sum_{j=1}^{T_m}c_{mj}^2
$$

代入整体loss后，有
$$
L_m=\sum_{i=1}^N[g_i\sum_{j=1}^{T_m}c_{mj}I(x_i\in R_j)+\frac{1}{2}h_i\sum_{j=1}^{T_m}c_{mj}^2I(x_i\in R_j)]+\gamma T_m+\frac{1}{2}\lambda \sum_{j=1}^{T_m}c_{mj}^2
$$
定义每个区域$R_j$的样本集合为$Ij$，所以
$$
L_m=\sum_{j=1}^{T_m}(\sum_{x_i\in I_j}g_i)c_{mj}+\frac{1}{2}\sum_{j=1}^{T_m}(\sum_{x_i\in I_j}h_i)c_{mj}^2+\gamma T_m+\frac{1}{2}\lambda \sum_{j=1}^{T_m}c_{mj}^2\\
=\sum_{j=1}^{T_m}[G_jc_{mj}+\frac{1}{2}(H_j+\lambda)c_{mj}^2]+\gamma T_m
$$
其中，
$$
G_j=\sum_{x_i\in I_j}g_i,\ H_j=\sum_{x_i\in I_j}h_i
$$
所以整体loss是$c_ {mj}$和$T_m$的函数（其余为定值），如果树的结构已经确定，即$T_m$已经确定，要使loss达到极小，对$c_{mj}$求偏导，并令导数为0
$$
0=\frac{\part L_m}{\part c_{mj}}=G_j+(H_j+\lambda)c_{mj}
$$
所以
$$
c_{mj}=-\frac{G_j}{H_j+\lambda}
$$
就得到了第m颗树的第j个叶节点的最佳预测值。

https://zhuanlan.zhihu.com/p/76002032这个有空看一下

### AdaBoost和Gradient Boost区别分析

本质区别是boost方法的不同。AdaBoost前面的弱学习器的作用是调整下一次迭代时数据集的权重，而Gradient Boost是将每次迭代的弱学习器都直接累加到整体的强学习器上，每次迭代都会使用前面所有累加的“部分强学习器”的信息。（再看）

## bagging

参考：https://www.cnblogs.com/pinard/p/6156009.html

<img src="https://images2015.cnblogs.com/blog/1042406/201612/1042406-20161204200000787-1988863729.png" alt="img" style="zoom:50%;" />



### 原理

Bootstrap aggregating

bagging的目的是降低方差，提高模型鲁棒性，即减小过拟合。所以bagging的**基础弱学习器应该是低偏差的强模型，即单个模型也可以取得较高的准确率**，但方差大，即容易过拟合，所以**bagging的目的就是降低过拟合**。

bagging的基础学习器可以采用任意的分类回归模型，一般选决策树或神经网络。

#### 抽样方法

每个基础学习器都使用不同的训练集来训练，做到不同的方法是对每个基础学习器，都**对训练集进行一次有放回的采样，且每个样本采样概率一致，采的样本个数与原始训练集一致**。**这样使得基础学习器之间尽可能的相互独立**。Bootstrap就是随机取样的意思。

#### 聚合方法

分类模型采用投票法，所有基础学习器分类输出的结果最多的类为最终输出。回归模型采用平均法。

### 随机森林

随机森林就是**以CART决策树为基础学习器**的bagging。

并且，基础学习器不是普通的CART决策树，相比于普通的决策树，随机森林的每棵树在生成（训练）时，**只随机选取其中一部分特征，而非全部特征都用来训练（对每一个内部节点都是如此，即每一个节点划分时，都从这个节点的特征集合中随机选一部分）**。设总特征数为$n$，选取的特征数为$n_{sub}$，$n_{sub}$越小，模型鲁棒性越强，即越能抑制过拟合。这是因为随机抽取一部分特征来训练，可以降低单个模型的方差，从而进一步降低整个方差，即降低了整个模型的过拟合程度。

注意，单个决策树生成的时候，因为做了特征随机选取，所以是不需要剪枝的。

总结一下，**随机森林就是以CART决策树为基础学习器的bagging算法，并且在每棵树生成过程进行随机特征选取**

#### 随机森林输出特征重要性

参考：https://www.jianshu.com/p/7a876bb876b9

（GBDT和XGBoost也有类似的输出特征重要性功能）

设随机森林为$RF$，随机森林中每一颗决策树为$RF_i$，$i=1,2,...,N$，共有N个树

设数据集共有D维特征，为$X_1,X_2,...,X_D$，每个特征对应的重要性分数为$VIM_d$，$d=1,2,...,D$

设决策树i的m号节点的不纯度（如基尼指数、平方误差）为$INP_{i,m}$

若决策树i的m号节点以特征$X_d$为划分节点，则有
$$
VIM_{d,i,m}=INP_{i,m}-INP_{i,m,l}-INP_{i,m,r}
$$
其中，$INP_{i,m,l}$和$INP_{i,m,r}$分别表示决策树i的m号节点的左子节点和右子节点的不纯度，这个式子就表示按选定的特征划分带来的不纯度的减少，减少的越多越好。

设决策树i以特征$X_d$为划分特征的节点的集合为$M$，则特征$X_d$在决策树i上的重要性分数为
$$
VIM_{d,i}=\sum_{m\in M}VIM_{d,i,m}
$$
特征$X_d$在整个随机森林$RF$上的重要性分数为
$$
VIM_d=\sum_{i=1}^NVIM_{d,i}=\sum_{i=1}^N\sum_{m\in M}VIM_{d,i,m}
$$
最后做归一化，得到最终的特征重要性分数
$$
VIM_d=\frac{VIM_d}{\sum_{d=1}^DVIM_d}
$$

#### 随机森林的优缺点

除了bagging自身的优缺点外，随机森林特有的优缺点：

- 优点
  1. 可以随机选择决策树节点划分特征，这样在样本特征维度很高的时候，仍然能高效的训练模型
  2. 对部分特征缺失不敏感，原理也是因为做了随机选择特征
  3. 能灵活处理各种类型的特征数据，不需要进行特征归一化（回归问题还是需要的），这是由决策树模型的特点决定的。
  4. **可以输出各个特征的重要性**。
- 缺点
  1. 在噪声比较大的情况下会过拟合。

### bagging的优缺点

- 优点
  1. 输出模型方差小，泛化能力强
  2. 可以并行实现，提高运算效率
  3. 实现比较简单
- 缺点
  1. 如果基础学习器数量过多，训练需要的时间和空间比较大
  2. 无法确定每个基础学习器的权重，只能简单的投票或平均

## stacking

参考：https://www.jiqizhixin.com/articles/2019-05-15-15

stacking和boosting、bagging最大的不同在于stacking聚合的模型是异质的。

stacking的算法步骤是：

1. 学习多个不同类型的基础学习器
2. 学习一个元模型，将基础学习器进行组合

具体算法流程为：

1. 将训练集$T=\{(x_1,y_1),(x_2,y_2),...,(x_N,y_N )\}$分为两组$T_1=\{(x_1,y_1),(x_2,y_2),...,(x_{N_1},y_{N_1})\},T_2=\{(x_1,y_1),(x_2,y_2),...,(x_{N_2},y_{N_2})\}$，其中$N_1+N_2=N$

2. 用第一组数据分别训练各个基础学习器

   设基础学习器为$f_1(x),f_2(x),...,f_ M(x)$，用$T_1$去训练这M个基础学习器
   $$
   f_m=\arg\min_{f_m}\sum_{i=1}^{N_1}Loss_{m}(y_i,f_m(x_i)),\ m=1,2,...,M
   $$

3. 用训练好的各个基础学习器预测第二组数据
   $$
   \hat y_{i,m}=f_m(x_i),\ m=1,2,...,M,\ i=1,2,...,N_2
   $$

4. 用第二组数据训练元模型，用各个基础学习器的输出作为元模型的输入

   最终的模型为
   $$
   g(f_1(x),f_2(x),...,f_M(x))
   $$
   $g$就是元模型，用来组合各个基础学习器

   用$T_2$来训练元模型
   $$
   g=\arg\min_g\sum_{i=1}^{N_2}Loss(y_i,g(f_1(x_i),f_2(x_i),...,f_M(x_i)))
   $$

元模型一般用神经网络。

## 推导boosting和bagging对于模型方差的变化

参考：https://zhuanlan.zhihu.com/p/86263786

对一个加法模型
$$
f(x)=\sum_{m=1}^M\beta_mb_m(x)
$$
对每一个基础学习器，设方差为$\sigma^2$，并假设两两基础模型的相关系数都为$\rho$

整体模型的方差为（公式参考https://yq.aliyun.com/articles/65262）
$$
Var(f)=Var(\sum_{m=1}^M\beta_mb_m(x))\\
=\sum_{m=1}^MVar(\beta_mb_m)+\sum_{i\ne j}\rho\beta_i\beta_j\sqrt{Var(b_i)}\sqrt{Var(b_j)}\\
=\sigma^2\sum_{m=1}^M\beta_m^2+\rho\sigma^2\sum_{i\ne j}\beta_i\beta_j
$$

### bagging的方差

对于bagging模型，由于采用的是简单的平均加权，所以$\beta_m=\frac{1}{M}$，所以bagging的方差为
$$
Var(f)=\sigma^2\sum_{m=1}^M\beta_m^2+\rho\sigma^2\sum_{i\ne j}\beta_i\beta_j\\
=\sigma^2M(\frac{1}{M})^2+\rho\sigma^2M(M-1)(\frac{1}{M})^2\\
=\frac{1+\rho(M-1)}{M}\sigma^2
$$
bagging聚合的各个基础模型之间是具有一定的相互独立性的（每个基础模型的训练集都是重新抽样所得），所以相关系数$\rho<1$，所以$\frac{1+\rho(M-1)}{M}<1$，所以$Var(f)<\sigma^2$，即相比于单个基础模型的方差，bagging整体模型的方差减小了，意味着过拟合更加被抑制了。

最理想情况下，各个基础模型之间完全独立，$\rho=0$，这时$Var(f)=\frac{1}{M}\sigma^2$

注意到随机森林算法里，在每棵树生成时做了随机特征选取，目的是降低单个模型的方差，通过公式可以发现，单个模型方差$\sigma^2$的降低，整体模型方差$Var(f)$也会随之降低。所以每棵树生成时的随机特征选取，也是为了降低整体模型方差。

### boosting的方差

这里假设boosting的每个弱学习器的权重都相等，所以可以得到和bagging一样的方差表示
$$
Var(f)=\frac{1+\rho(M-1)}{M}\sigma^2
$$
由于boosting每个弱学习器之间都有很强的关联，因为每个弱学习器都受到前面学习器的影响，所以可以近似$\rho=1$，得到
$$
Var(f)=\sigma^2
$$
即整体模型的方差和单个弱学习器的方差是一样的，即没有或很少有降低方差作用。这也是boosting要求弱学习器为简单低方差模型的原因。

