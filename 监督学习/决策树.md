# 决策树

## 核心概述

决策树一般是针对分类问题，且每个特征的取值集合是离散的。不过也有回归树，特征也可以是连续的。下面主要叙述的决策树都是分类树，特征是离散的。

既然是树，那么就是由多个节点组成的，节点又可以分为内部节点和叶节点。在决策树中，每个内部节点代表一个特征，叶节点代表分类结果label。

<img src="https://pic3.zhimg.com/80/v2-8f6407e5ab5a58b2913aef6a332090f6_720w.jpg" alt="img" style="zoom:50%;" />

### 决策树的推理过程

对于一个生成好并经过剪枝的决策树，决策树的每条路径（从根节点到叶节点）代表一条分类规则，对一个输入的实例，如果这个实例的特征满足某条路径的要求，那么就输出这条路径叶节点表示的分类结果。

## 决策树生成

### 决策树生成的原理

决策树的生成就是决策树模型的训练过程。

在决策树训练过程中，每个内部节点不仅仅表示一个特征，同时还表示训练数据的划分集合，即**训练过程中的内部节点表示特征+训练数据集合**。

初始时，根节点表示全部训练集，找到一个“最优特征”存入这个节点，并将节点内的训练数据按照这个特征的取值进行划分，划分的子集存入这个节点的子节点中，再对每个子节点重复上面的步骤进行迭代，若一个子节点表示的训练数据都属于一个类别，则这个子节点成为叶节点，停止迭代。即**每次迭代针对一个内部节点，这个内部节点的数据由上层节点根据特征划分产生，再对这个节点的数据，找到一个“最优特征”，进行划分，生成下一层节点，就这样迭代下去**。**最终生成的决策树，每个内部节点只保留特征的表示，用来对测试数据进行分类。**

那么最直接的问题就是如何找到“最优特征”，就是特征选择。

### 特征选择

使用信息增益(比)或基尼指数来衡量特征是否最优，用熵和条件熵来表示信息增益。

**特征选择的核心原则：特征所带来的不确定性越小越好。**

#### 熵和条件熵

设随机变量X的分布为
$$
P(X=x_i)=p_i, \ \ i=1,2,...,n
$$
则X的熵为
$$
H(X)=-\sum_{i=1}^np_i\log_2p_i
$$
熵越大，表示不确定性越大，所蕴含的信息量越大。

条件熵为
$$
H(Y|X)=\sum_{i=1}^np_iH(Y|X=x_i)
$$
其中，$p_i=P(X=x_i),\ i=1,2,...,n$

条件熵表示在已经X的条件下，Y的不确定性。

采用数据估计（极大似然估计）得到的分布，进而得到的熵称作经验熵及经验条件熵。

#### 信息增益

设**当前迭代针对的内部节点**，所包含的数据为$D$，数据样本数为$|D|$

特征A在数据集D上的信息增益为
$$
g(D,A)=H(D)-H(D|A)
$$
熵是不确定性，$H(D)$表示整个数据集分类的不确定性，$H(D|A)$表示在特征A确定条件下数据集分类的不确定性，**二者的差表示特征A对数据集分类不确定性的降低程度**，差越大，降低程度越大，表示特征A对于数据集分类有更大的作用，即是更优的特征。降低程度也可以叫做信息增益。

下面的问题就是如何求信息增益，需要求熵和条件熵，采用的方法就是极大似然估计，即得到经验熵和经验条件熵。

有K个类别$C_k$，$k=1,2,...,K$，每个类别的样本数为$|C_k|$，有$\sum_{k=1}^K|C_k|=|D|$

设特征A有n个可能的取值$a_1,a_2,...,a_n$，根据每个取值所划分的子集为$D_1,D_2,...,D_n$，子集$D_i$的样本数为$|D_i|$

记子集$D_i$中属于类别$C_k$的数据集合为$D_{ik}$，即$D_{ik}=D_i\cap C_k$，集合样本数为$|D_{ik}|$

极大似然估计的过程可以参照[贝叶斯模型](E:\研究生\秋招复习\机器学习\ML\监督学习\贝叶斯模型.md)中的推导，这里直接**用极大似然估计就是求频率的思路**

D的分布为
$$
P(D=C_k)=\frac{|C_k|}{|D|}
$$
所以D的熵为
$$
H(D)=-\sum_{k=1}^K\frac{|C_k|}{|D|}\log_2\frac{|C_k|}{|D|}
$$
D关于A的条件概率分布为
$$
P(D=C_k|A=a_i)=\frac{|D_{ik}|}{|D_i|}
$$
A的分布为
$$
P(A=a_i)=\frac{|D_i|}{|D|}
$$
$D|A=a_i$的熵为
$$
H(D|A=a_i)=-\sum_{k=1}^K\frac{|D_{ik}|}{|D_i|}\log_2\frac{|D_{ik}|}{|D_i|}
$$
所以$D|A$的熵为
$$
H(D|A)=-\sum_{i=1}^n\frac{|D_i|}{|D|}\sum_{k=1}^K\frac{|D_{ik}|}{|D_i|}\log_2\frac{|D_{ik}|}{|D_i|}
$$

#### 信息增益比

如果用信息增益来衡量，那么就会偏向于选择可能取值数多的特征，因为取值数越多会导致划分出更多的子集，进而导致条件熵更小。平衡的方法是**除特征A的熵**，取值数越多，就会导致A的不确定性越大，熵越大。（这个地方理论推导还要再看）

特征A的熵为
$$
H_D(A)=-\sum_{i=1}^n\frac{|D_i|}{|D|}\log_2\frac{|D_i|}{|D|}
$$
信息增益比定义为
$$
g_R(D,A)=\frac{g(D,A)}{H_D(A)}
$$
信息增益比最大的特征可视为最优特征。

#### 基尼指数

分类问题中，设有K个类，样本属于第k类的概率为$p_k$，则概率分布的基尼指数为
$$
Gini(p)=\sum_{k=1}^Kp_k(1-p_k)=1-\sum_{k=1}^Kp_k^2
$$
对于给定的训练集D，其基尼指数为
$$
Gini(D)=1-\sum_{k=1}^K\frac{|C_k|}{|D|}
$$
符号含义和上面一致。

**基尼指数这一特征选择方法在CART决策树生成算法中使用，CART决策树是一个二叉树，即对每一个特征，找一个可能的取值，按是否等于这个取值，将数据集划分为两个子集，分别放入两个子节点中，形成二叉树。**

设训练集D的一个特征为A，A的一个可能取值为a，那么根据特征A的取值是否为a，可将D划分为两部分
$$
D_1=\{x|x^{(A)}=a\},\ D_2=D-D_1
$$
在特征A条件下，D的基尼指数为
$$
Gini(D,A)=\frac{|D_1|}{|D|}Gini(D_1)+\frac{|D_2|}{|D|}Gini(D_2)
$$
更一般的，**基尼指数取决于特征A和其取值划分点a这两个值**，所以重新定义如下
$$
D_{1a}=\{x|x^{(A)}=a\},\ D_{2a}=D-D_{1a}
$$

$$
Gini(D,A=a)=\frac{|D_{1a}|}{|D|}Gini(D_{1a})+\frac{|D_{2a}|}{|D|}Gini(D_{2a})
$$

基尼指数和熵是相似的，**基尼指数越大，代表数据集的不确定性越大**。

$Gini(D)$表示数据集的不确定性，$Gini(D,A=a)$表示经特征A=a条件分割后的不确定性。基尼指数最小的特征即为最优特征，并按照最优取值划分点进行划分，即
$$
\arg\min_{A,a}Gini(D,A=a)
$$

### 决策树生成的具体算法

#### ID3

输入：训练集D，D有n维特征，所有特征的集合为A。阈值$\epsilon$

输出：决策树$T$

算法：

1. 若D中所有样本均属于同一个类别$C_k$，则$T$为单节点（根节点就是叶节点）树，以$C_k$作为类label，返回$T$
2. 若$A=\empty$，则$T$为单节点（根节点就是叶节点）树，类label为$\arg\max_k|C_k|$，即样本数最多的类，返回$T$
3. 计算每一个特征的信息增益，选择信息增益最大的特征$A_g$
4. 若$A_g$的信息增益小于阈值$\epsilon$，则$T$为单节点（根节点就是叶节点）树，类label为$\arg\max_k|C_k|$，即样本数最多的类，返回$T$
5. 否则，根据$A_g$的取值，将D划分为多个子集$D_i$，并构建子节点，将每个子集放到对应的子节点里，并将$A_g$存入当前的节点
6. 对第i个子节点，以$D_i$为训练集，$A-\{A_g\}$为特征集，递归调用1-5步，得到子树$T_i$

#### C4.5

和ID3大致一样，只是第三步求的是信息增益比。

#### CART

输入：训练集D，特征集合A

输出：二叉树型决策树$T$

算法：

1. 计算每一个特征及其对应的所有可能取值的基尼指数，选择基尼指数最小的特征$A_m$及对应的划分点$A_m=a$
2. 将训练集D按照$A_m=a$与否划分成两个子集$D_1,D_2$，并分别放到两个子节点里，并将$A_m=a$存入当前节点
3. 对第1个子节点，以$D_1$为训练集，$A-\{A_m\}$为特征集，递归调用1-2步，得到子树$T_1$
4. 对第2个子节点，以$D_2$为训练集，$A-\{A_m\}$为特征集，递归调用1-2步，得到子树$T_2$

算法终止条件为：节点中的样本个数小于阈值；$Gini(D)$小于阈值，即样本基本属于同一类；$A=\empty$

## 决策树剪枝

训练集生成的决策树对训练集分类肯定没问题，但对测试数据就不那么好了，就是过拟合了。过拟合的原因就是模型（决策树）太复杂了，所以要对树进行剪枝。

**剪枝的核心思路是极小化加上正则化的loss**。

设决策树$T$的叶节点个数为$|T|$，训练数据的预测误差为$C(T)$，则整体loss为
$$
C_\alpha(T)=C(T)+\alpha|T|
$$
其中$\alpha$为正则化参数，**正则化的方法就是对叶节点个数进行限制**。也就是降低模型复杂度。

$C(T)$的具体形式可以是熵或者基尼指数，不再赘述。（有空再补）

### 一般剪枝方法

设一组叶节点（属于同一个父节点）回缩到其父节点之前和之后的决策树分别为$T_B$和$T_A$，对应的整体loss分别为$C_\alpha(T_B)$和$C_\alpha(T_A)$，如果$C_\alpha(T_A)<=C_\alpha(T_B)$，则进行剪枝，即把这一组叶节点全剪掉。

注：回缩之后父节点成为了叶节点，其label为样本数最多的类的label

### CART剪枝

有空再补

## 回归树

上面所说的都是针对分类问题，且特征取值离散。回归树则是针对回归问题，且特征取值连续。（分类问题但特征连续，回归问题但特征离散的情况暂不考虑，这个部分放到特征工程里细说）

回归树一般是CART回归树算法生成。

### CART回归树的特征选择

CART回归树和CART分类树一样，也是二叉树。划分方法和分类树类似，只不过根据特征连续的特点，用大于划分点和小于划分点来进行划分。

设训练集为$D=\{(x_1,y_1),(x_2,y_2),...,(x_n,y_n)\}$

选择训练集第j个特征$x^{(j)}$，及其一个取值s，作为划分特征和划分点，将数据集D划分成两个子区域
$$
R_1(j,s)=\{x|x^{(j)}<=s\},\ R_2(j,s)=\{x|x^{(j)}>s\}
$$
两个子区域对应的输出分别为$c_1,c_2$，这里定义每个子区域都有一个固定的输出值。

求区域输出值的方法采用最小二乘策略（所以也叫最小二乘回归树），即
$$
\hat c_1=\arg\min_{c_1}\sum_{x_i\in R_1}(y_i-c_1)^2
$$
对$c_1$求导，并令导数为0，可得
$$
0=-2\sum_{x_i\in R_1}(y_i-c_1)
$$
所以
$$
\hat c_1=\frac{1}{N_1}\sum_{x_i\in R_1}y_i
$$
其中，$N_1$为子区域$R_1$中的样本数，即用最小二乘求出的输出值为该区域输出label的平均值。

同理，对$c_2$，有
$$
\hat c_2=\arg\min_{c_2}\sum_{x_i\in R_2}(y_i-c_2)^2
$$
得到
$$
\hat c_2=\frac{1}{N_2}\sum_{x_i\in R_2}y_i
$$
以两个子区域的平方误差的和作为衡量当前划分特征和划分点的指标，即
$$
square\_sum(j,s)=\sum_{x_i\in R_1(j,s)}(y_i-\hat c_1)+\sum_{x_i\in R_2(j,s)}(y_i-\hat c_2)
$$
最优的划分特征和划分点为
$$
\arg\min_{j,s}square\_sum(j,s)=\arg \min_{j,s}[\sum_{x_i\in R_1(j,s)}(y_i-\hat c_1)+\sum_{x_i\in R_2(j,s)}(y_i-\hat c_2)]
$$

### CART回归树生成具体算法

输入：数据集D

输出：回归树$f(x)$

算法：

1. 遍历所有特征j，对每个特征遍历划分点s，求出最优划分特征和划分点$(j,s)$
   $$
   \arg\min_{j,s}square\_sum(j,s)=\arg \min_{j,s}[\sum_{x_i\in R_1(j,s)}(y_i-\hat c_1)+\sum_{x_i\in R_2(j,s)}(y_i-\hat c_2)]
   $$

2. 用选定的$(j,s)$将D划分为两个子区域$R_1,R_2$
   $$
   R_1(j,s)=\{x|x^{(j)}<=s\},\ R_2(j,s)=\{x|x^{(j)}>s\}
   $$
   及对应的输出值
   $$
   \hat c_1=\frac{1}{N_1}\sum_{x_i\in R_1}y_i\\
   \hat c_2=\frac{1}{N_2}\sum_{x_i\in R_2}y_i
   $$
   将两个子区域的数据集$D_1, D_2$及对应的输出值$\hat c_1,\hat c_2$分别放到两个子节点中，并将$(j,s)$存入当前节点

3. 对第1个子节点，以$D_1$为训练集，并排除当前选择的最优特征，递归调用1-2步，得到子树$T_1$

4. 对第2个子节点，以$D_2$为训练集，并排除当前选择的最优特征，递归调用1-2步，得到子树$T_2$

最终，将输入空间划分为M个区域$R_1,R_2,...,R_M$（递归完成后，有M个叶节点），每个区域对应的输出值为$\hat c_m$，生成决策树为
$$
f(x)=\sum_{m=1}^M\hat c_mI(x\in R_m)
$$
推理过程中，直接按照这个式子计算输出值即可。



