# 贝叶斯模型

## 朴素贝叶斯法

### 生成模型与判别模型

参考：https://www.jianshu.com/p/4ef549eb0ad4

这部分还需要再看。

有监督机器学习的本质是学习输出关于输入的条件概率分布$P(Y|X)$，怎么学习这个条件概率分布，有判别模型和生成模型两种方法。

- 判别模型

  直接学习$P(Y|X)$

- 生成模型

  先学习联合概率分布$P(X,Y)$，再根据贝叶斯公式，
  $$
  P(Y|X)=\frac{P(X,Y)}{P(X)}
  $$
  朴素贝叶斯法就是典型的生成模型。其他还有隐马尔科夫模型。

  生成模型适用于大数据量的数据集，因为小数据量学到的联合分布没有说服力。

### 朴素贝叶斯法推导

设训练数据集为$T=\{(x_1,y_1),(x_2,y_2),...,(x_N,y_N)\}$

其中输入数据$x_i \in R^D,x_i=[x_i^{(1)},x_i^{(2)},...,x_i^{(D)}]$，输出label $y_i\in R,y_i\in\{c_1,c_2,...,c_K\}$，$i=1,2,...,N$

由于朴素贝叶斯是生成模型，所以
$$
P(Y|X)=\frac{P(X,Y)}{P(X)}\\
=\frac{P(X|Y)P(Y)}{P(X)}\\
=\frac{P(X=x|Y=c_k)P(Y=c_k)}{P(X=x)}\\
=\frac{P(X^{(1)}=x^{(1)},X^{(2)}=x^{(2)},...,X^{(D)}=x^{(D)}|Y=c_k)P(Y=c_k)}{P(X^{(1)}=x^{(1)},X^{(2)}=x^{(2)},...,X^{(D)}=x^{(D)})}
$$

#### 朴素贝叶斯法的条件独立性假设（特征独立性）

条件概率分布$P(X=x|Y=c_k)$参数量过大，因为对于共有D维的X，每一维都可能有很多个取值，组合起来就太多了。所以朴素贝叶斯这里就进行了一个假设，**假设每一维特征之间都是相互独立的**。即
$$
P(X=x|Y=c_k)=P(X^{(1)}=x^{(1)},X^{(2)}=x^{(2)},...,X^{(D)}=x^{(D)}|Y=c_k)\\
=\prod_{j=1}^DP(X^{(j)}=x^{(j)}|Y=c_k)
$$
这是一个很强的假设，损失了一定的准确率，所以叫朴素。

继续推导$P(Y|X)$，代入条件独立性假设，对分母用全概率公式，有
$$
P(Y|X)=\frac{P(Y=c_k)\prod_{j=1}^DP(X^{(j)}=x^{(j)}|Y=c_k)}{\sum_kP(Y=c_k)\prod_{j=1}^DP(X^{(j)}=x^{(j)}|Y=c_k)}
$$

#### 测试过程

对于输入的样本x，则输出的label y为
$$
y=\arg \max_{c_k}P(Y=c_k|X=x)
$$
即取最有可能属于的label

又对同一个输入x，分母的值是固定的，所以只需取分子最大即可
$$
y=\arg \max_{c_k}P(Y=c_k|X=x)\\
=\arg\max_{c_k}P(Y=c_k)\prod_{j=1}^DP(X^{(j)}=x^{(j)}|Y=c_k)
$$

#### 训练过程

需要根据训练集求出计算条件概率分布所需要的
$$
P(Y=c_k),\ \ \ P(X^{(j)}=x^{(j)}|Y=c_k)
$$
其中，$k=1,2,...,K$，$j=1,2,...,D$

### 朴素贝叶斯法的训练方法（参数估计）

训练即根据训练集求出
$$
P(Y=c_k),\ \ \ \prod_{j=1}^DP(X^{(j)}=x^{(j)}|Y=c_k)
$$
求解方法有两种：1. 极大似然估计；2. 贝叶斯估计

#### 极大似然估计

##### 1. 求先验概率$P(Y=c_k)$

设$P(Y=c_k)=p$，则$P(Y\neq c_k)=1-p$
$$
P(y_i|p)=p^{I(y_i=c_k)}(1-p)^{1-I(y_i=c_k)}
$$
其中$I$为0,1取值，满足相等条件取1，不然取0

Y的似然函数为
$$
P(Y|p)=\prod_{i=1}^Np^{I(y_i=c_k)}(1-p)^{1-I(y_i=c_k)}
$$
对数似然函数为
$$
L(p)=\log P(Y|p)=\sum_{i=1}^N[I(y_i=c_k)\log p+(1-I(y_i=c_k))\log (1-p)]
$$
对p求导，有
$$
\frac{\part L(p)}{\part p}=\sum_{i=1}^N[\frac{I(y_i=c_k)}{p}-\frac{1-I(y_i=c_k)}{1-p}]\\
=\frac{\sum_{i=1}^NI(y_i=c_k)}{p}-\frac{N-\sum_{i=1}^NI(y_i=c_k)}{1-p}
$$
令导数为0，可得
$$
p=\frac{\sum_{i=1}^NI(y_i=c_k)}{N}
$$
即得到
$$
P(Y=c_k)=\frac{\sum_{i=1}^NI(y_i=c_k)}{N}
$$

##### 2. 求条件概率$P(X^{(j)}=x^{(j)}|Y=c_k)$

设$P(X^{(j)}=x^{(j)}|Y=c_k)=p$，则$P(X^{(j)}\neq x^{(j)}|Y=c_k)=(1-p)$
$$
p(x_i^{(j)}|y_i=c_k,p)=p^{I(x_i^{(j)}=x^{(j)},y_i=c_k)}(1-p)^{1-I(x_i^{(j)}=x^{(j)},y_i=c_k)}
$$
$X|Y=c_k$的似然函数为
$$
P(X|Y=c_k,p)=\prod_{i=1}^{\sum_{i=1}^NI(y_i=c_k)}p^{I(x_i^{(j)}=x^{(j)},y_i=c_k)}(1-p)^{1-I(x_i^{(j)}=x^{(j)},y_i=c_k)}
$$
对数似然函数为
$$
L(p)=\log P(X|Y=c_k,p)\\
=\sum_{i=1}^{\sum_{i=1}^NI(y_i=c_k)}I(x_i^{(j)}=x^{(j)},y_i=c_k)\log p+(1-I(x_i^{(j)}=x^{(j)},y_i=c_k))\log(1-p)
$$
对p求导，有
$$
\frac{\part L(p)}{\part p}=\sum_{i=1}^{\sum_{i=1}^NI(y_i=c_k)}[\frac{I(x_i^{(j)}=x^{(j)},y_i=c_k)}{p}-\frac{(1-I(x_i^{(j)}=x^{(j)},y_i=c_k))}{1-p}]\\
=\frac{\sum_{i=1}^NI(x_i^{(j)}=x^{(j)},y_i=c_k)}{p}-\frac{\sum_{i=1}^NI(y_i=c_k)-\sum_{i=1}^NI(x_i^{(j)}=x^{(j)},y_i=c_k)}{1-p}
$$
令导数为0，可得
$$
p=\frac{\sum_{i=1}^NI(x_i^{(j)}=x^{(j)},y_i=c_k)}{\sum_{i=1}^NI(y_i=c_k)}
$$
即得到
$$
P(X^{(j)}=x^{(j)}|Y=c_k)=\frac{\sum_{i=1}^NI(x_i^{(j)}=x^{(j)},y_i=c_k)}{\sum_{i=1}^NI(y_i=c_k)}
$$

##### 从频率角度

可以发现**用最大似然估计求出的结果实际上就是频率**，这也是极大似然估计被叫做频率派的原因。

$P(Y=c_k)$实际就是求训练集中label为$c_k$的样本数占总样本数的比例。

$P(X^{(j)}=x^{(j)}|Y=c_k)$实际就是针对训练集中label为$c_k$的所有样本，求这些样本中满足$X^{(j)}=x^{(j)}$的样本所占的比例。

#### 贝叶斯估计

用贝叶斯估计法推导，这个方法再看吧。

用极大似然估计有可能会出现根据训练集**求出的两个概率为0的情况，会影响后面测试过程**。

贝叶斯法根据训练集求出的先验和条件两个概率，形式上和极大似然估计求出的类似，只不过多加了一个参数
$$
P(Y=c_k)=\frac{\sum_{i=1}^NI(y_i=c_k)+\lambda}{N+K\lambda}
$$
可以保证$P(Y=c_k)>0$，且$\sum_{k=1}^KP(Y=c_k)=1$
$$
P(X^{(j)}=x^{(j)}|Y=c_k)=\frac{\sum_{i=1}^NI(x_i^{(j)}=x^{(j)},y_i=c_k)+\lambda}{\sum_{i=1}^NI(y_i=c_k)+S_j\lambda}
$$
其中，$S_j$为输入X第j个维度可能取到的值的个数

可以保证$P(X^{(j)}=x^{(j)}|Y=c_k)>0$，且$\sum_{l=1}^{S_j}P(X^{(j)}=x^{(jl)}|Y=c_k)=1$

