# ML中的降维算法

## PCA

参考：百面机器学习

### PCA推导——最大化投影方差

PCA核心思想：**找到数据的主成分，利用主成分表征原始数据，从而达到降维**。  

以二维降到一维为引子来推导。  

二维表示平面，一维表示直线，也就是说要把分布在二维平面上的点都映射到一条直线上，那么怎么选这条直线呢？

<img src="https://images2015.cnblogs.com/blog/1042406/201612/1042406-20161231162149992-1521335659.png" alt="img"  />

首先要说明，二维点映射到直线的方法是将这个二维点投影到这条直线上，也就是做內积。

**我们希望，投影到这条直线上的数据分布应该越分散越好，即要有尽可能大的方差**。因为信号处理领域，信号具有较大的方差，而噪声具有较小的方差，即方差越大，表现出的信息量越大。

推广到一般维度的情况，

设有一组数据点${v_1,v_2,...,v_n}$，$v_i \in R^m$，即n个数据，每个数据有m维特征。目标是把维度降维到d维。

先对这组数据做中心化处理，即减去平均值
$$
\mu = \frac{1}{n}\sum_{i=1}^n v_i \\
x_1,x_2,...,x_n = v_1-\mu,v_2-\mu,...,v_n-\mu
$$
设一个m维的超平面为$w$，$w\in R^m$，我们希望中心化后的数据点在$w$超平面上的投影尽可能分散，即最大化投影方差。注意这里$w$为超平面的单位法向向量。

第i个数据点的投影坐标可以表示为內积形式
$$
(x_i,w)=x_i^Tw
$$
所有n个数据点的投影坐标均值$\mu'$为
$$
\mu'=\frac{1}{n}\sum_{i=1}^n x_i^Tw=\frac{1}{n}(\sum_{i=1}^n(v_i-\mu)^T)w=(\frac{1}{n}\sum_{i=1}^nv_i^T-\mu^T)w=0
$$
投影坐标均值为0，这也是做去中心化的意义所在。

n个数据点的投影坐标方差为
$$
D(x)=\frac{1}{n}\sum_{i=1}^n(x_i^Tw-\mu')^2=\frac{1}{n}\sum_{i=1}^n(x_i^Tw)^2 \\
=\frac{1}{n}\sum_{i=1}^n(x_i^Tw)^T(x_i^Tw) \\
=\frac{1}{n}\sum_{i=1}^nw^Tx_ix_i^Tw \\
=w^T(\frac{1}{n}\sum_{i=1}^nx_ix_i^T)w
$$
其中，$\frac{1}{n}\sum_{i=1}^nx_ix_i^T$为样本协方差矩阵，记做$C$，且$w$为单位向量，由此，可以引出最大化$D(x)$的优化问题，表示为：
$$
\begin{equation}
\left\{
\begin{array}{lr}max\ \ w^T\ C\ w \\
s.t.\ \ w^Tw=1
\end{array}
\right.
\end{equation}
$$
使用拉格朗日乘子法求解这个优化问题，（注意这里变量是$w$）
$$
F(w,\lambda)=w^TCw-\lambda(w^Tw-1)\\
\frac{\partial F}{\partial w}=0 \\
\frac{\partial F}{\partial \lambda}=0
$$
其中，
$$
\frac{\partial F}{\partial w}=\frac{\partial (w^TCw-\lambda(w^Tw-1))}{\partial w}\\
=(C+C^T)w-2\lambda w \\
=2Cw-2\lambda w
$$
所以，有$Cw=\lambda w$，代入到$D(x)$表达式，有
$$
D(x)=w^TCw=w^T\lambda w=\lambda w^Tw=\lambda
$$
所以$D(x)$的最大值就是$\lambda$，而又因为$Cw=\lambda w$，**可知$\lambda$为协方差矩阵C的特征值**，**所以$D(x)$能取到的最大值就是协方差矩阵C的最大特征值**，**取到最大值时的$w$就是最大特征值对应的特征向量**。

由此，便找到了使数据点投影方差最大的一个超平面。

但是，如果只用这一个超平面，我们就会将数据降维到了一维，就会有比较多的信息损失，而且与我们降维到d维的要求不符，所以我们还需要再选取其他d-1个超平面，这d-1个超平面就是第2大，第3大，...，一直到第d-1大的特征值所对应的特征向量。由此，得到d个超平面，数据点在每个超平面的投影坐标是一维数据，d个加起来就是d维数据。设降维后的第i个数据点为$x_i,x_i'\in R^d$，则有
$$
x_i'=(w_1^Tx_i,w_2^Tx_i,...w_d^Tx_i)^T
$$
写成矩阵形式，为
$$
[x_1',x_2',...,x_n'] = 
\left[
\begin{matrix}
w_1^T\\
w_2^T\\
...\\
w_d^T
\end{matrix}
\right]
[x_1,x_2,...,x_n]
$$
注，样本协方差矩阵也可以写成样本矩阵的表示形式，设X为样本矩阵
$$
X=(x_1,x_2,...,x_n) \\
X^T=(x_1^T,x_2^T,...,x_n^T)^T
$$
协方差矩阵C就表示为
$$
C=\frac{1}{n}XX^T
$$
再定义一个降维后的信息占比，即信息保留度为
$$
\sqrt{\frac{\sum_{i=1}^d \lambda_i^2}{\sum_{i=1}^n \lambda_i^2}}
$$
其中，$\lambda$为协方差矩阵的特征值。

### PCA推导——最小平方误差

### PCA算法流程总结

1. 对样本数据点进行中心化处理，即减去平均值
2. 求样本的协方差矩阵
3. 求协方差矩阵的特征值和特征向量，将特征值从大到小排序
4. 取前d个特征值对应的特征向量，每个数据点与这d个特征向量分别做內积，得到d维向量，即降维后的结果（或者直接说将d个特征向量构造为变换矩阵，对原始数据点变换后得到降维后的结果）

### PCA与SVD

参考：https://www.cnblogs.com/pinard/p/6251584.html

PCA算法第三步需要求样本协方差矩阵的特征向量，这是需要很大计算量的，尤其对维度很高样本数也很多的样本。这里就需要用到SVD来优化计算。

#### SVD定义

对于方阵$A\in R^{n*n}$，可以通过求其特征值和特征向量对其进行特征分解
$$
A=W\ \sum \ W^T
$$
其中，$\sum$为对角阵，对角线元素为A的n个特征值，W是n个特征向量张成的矩阵，每个特征向量均为单位向量，且两两正交，有$W^TW=I$。

如果$A\in R^{m*n}$不是方阵，就无法求A的特征值和特征向量，那么有没有办法对非方阵A进行特征分解呢？方法就是SVD。

定义A的SVD特征分解为
$$
A=U\ \sum\ V^T
$$
其中，$\sum\in R^{n*n}$为对角阵，对角线元素称作A的奇异值。$U\in R^{m*m}$，$V\in R^{n*n}$，U和V都为酉矩阵，即$U^TU=I$，$V^TV=I$。

求解U，V的方法：

$A^TA\in R^{n*n}$为n阶方阵，可以直接进行特征分解，将$A^TA$的所有特征向量张成一个矩阵，就是矩阵V

$AA^T\in R^{m*m}$为m阶方阵，可以直接进行特征分解，将$AA^T$的所有特征向量张成一个矩阵，就是矩阵U

[注意]上面所说的特征向量都是列向量，张成的矩阵U和V的每一列是一个特征向量

求解$\sum$的方法：
$$
A=U\ \sum\ V^T \\
A^T = V\ \sum\ ^T\ U^T=V\ \sum\ U^T\\
AA^T=U\ \sum\ V^TV\ \sum\ U^T=U\ \sum\ ^2U^T
$$
可以发现$AA^T$的特征值是$\sum$矩阵对角线元素（也就是奇异值）的平方，所以就可以用$AA^T$的特征值求平方根求出$\sum$。

#### SVD用于PCA

PCA算法求解过程中，样本矩阵设为$X\in R^{m*n}$，其中m为维度，n个样本数。样本协方差矩阵为$C=\frac{1}{n}XX^T$，需要求解C的特征向量。

可以发现，在SVD中，协方差矩阵$C$的特征向量矩阵就是样本矩阵$X$的U矩阵，所以只要能直接得到$X$的U矩阵，就可以避免协方差矩阵的复杂计算。而有一些SVD实现算法是不需要通过计算$AA^T$的，可以直接求出来U矩阵，所以，PCA算法中第三步求协方差矩阵的特征向量，就可以直接用这种高级的SVD实现算法来替代，减少了运算量。

### PCA优缺点

#### 优点

1. 仅仅需要以方差衡量信息量，不受数据集以外的因素影响
2. 降维后各维度之间互相正交，消除了各维度之间的影响（协方差矩阵的各个特征向量是两两正交的）
3. 计算方法简单

#### 缺点

1. 各个特征维度的含义可解释性不强
2. 不适合对非高斯分布样本进行降维（不太懂）
3. 线性降维具有局限性，可以通过核映射得到核主成分分析KPCA

## LDA

参考：

https://www.cnblogs.com/pinard/p/6244265.html （多分类部分看自己的，这个链接有理解错误）

https://shengtao96.github.io/2017/06/20/Linear-Discriminant-Analysis/

### LDA推导

PCA是一种无监督的降维算法，LDA是有监督的降维算法，即LDA处理的数据点是具有类别标签的。

LDA的核心思想：**降维后数据点类内方差最小，类间方差最大**。也可以说是**类内距离最小，类间距离最大**（熟悉的人脸识别经典语句啊）

#### 二分类情况

设有$C_1,C_2$两个类别的样本，两类的均值分别为
$$
\mu_1=\frac{1}{N_1}\sum_{x\in C_1}x \\
\mu_2=\frac{1}{N_2}\sum_{x\in C_2}x
$$
其中，$x\in R^m$，维度为m

设投影到的超平面为$w\in R^m$，

投影后的类间距离表示为
$$
D = ||w^T\mu_1-w^T\mu_2||_2^2=||w^T(\mu_1-\mu_2)||_2^2=w^T(\mu_1-\mu_2)(\mu_1-\mu_2)^Tw
$$
类内方差（距离）表示为
$$
D_1 = \sum_{x\in C_1}(w^Tx-w^T\mu_1)^2=\sum_{x\in C_1}w^T(x-\mu_1)(x-\mu_1)^Tw \\
D_2 = \sum_{x\in C_2}(w^Tx-w^T\mu_2)^2=\sum_{x\in C_2}w^T(x-\mu_2)(x-\mu_2)^Tw
$$
根据类内类间核心思想，优化目标可以表示为
$$
max_wJ(w)=\frac{D}{D_1+D_2}=
\frac{w^T(\mu_1-\mu_2)(\mu_1-\mu_2)^Tw}{w^T[\sum_{x\in C_1}(x-\mu_1)(x-\mu_1)^T+\sum_{x\in C_2}(x-\mu_2)(x-\mu_2)^T]w}
$$
记
$$
S_B=(\mu_1-\mu_2)(\mu_1-\mu_2)^T\\
S_W=\sum_{x\in C_1}(x-\mu_1)(x-\mu_1)^T+\sum_{x\in C_2}(x-\mu_2)(x-\mu_2)^T
$$
则优化目标可以写为
$$
max_wJ(w)=\frac{w^TS_Bw}{w^TS_Ww}
$$
这是广义瑞利商的形式，结论为：$J(w)$的最大值为矩阵$S_w^{-1}S_B$的最大特征值，取到最大值时的$w$为最大特征值对应的特征向量。



关于这个结论，简单的一个推导：

先对分子$w^TS_Ww$做归一化，即令$||w^TS_Ww||=1$，（这样做是合理的，但是还需要再理解）

用拉格朗日乘子法
$$
F(w,\lambda)=w^TS_Bw-\lambda(w^TS_Ww-1) \\
\frac{\partial F}{\partial w}=(S_B+S_B^T)w-\lambda(S_W+S_W^T) w=2S_Bw-2\lambda S_Ww=0
$$
得到，$S_Bw=\lambda S_Ww$，即，$S_W^{-1}S_Bw=\lambda w$。得证。



根据$S_B=(\mu_1-\mu_2)(\mu_1-\mu_2)^T$，$S_Bw=(\mu_1-\mu_2)(\mu_1-\mu_2)^Tw$，后面两项相乘是个数值，所以$S_Bw$的方向始终与$(\mu_1-\mu_2)$保持一致。又根据$S_W^{-1}S_Bw=\lambda w$，所以$w$方向始终与$S_w^{-1}(\mu_1-\mu_2)$保持一致，不考虑数值大小只考虑方向的话，可以直接令$w=S_w^{-1}(\mu_1-\mu_2)$。这就意味着，对于二分类LDA，只要求出类内方差矩阵（两类各自的协方差矩阵之和）和两类的均值，就可以得出最佳降维投影超平面。

降维结果为
$$
x'=w^Tx
$$
还可以发现，二分类LDA只能降维到1维。

#### 多分类情况

设有$C_1,C_2,...,C_k$共k个类别的样本，每一类样本维度为m维，均值为$\mu_i$，样本数为$N_i$。目标是降成d维。

针对多分类的优化目标，相比二分类的，类内方差矩阵$S_W$只需要由两个类内方差和变成k个类内方差和即可，
$$
S_W=\sum_{i=1}^k\sum_{x\in C_i}(x-\mu_i)(x-\mu_i)^T
$$
其实就是k个类的协方差矩阵相加，二分类也一样。

类间方差的建模则要进行修改，改为
$$
S_B=\sum_{i=1}^kN_i(\mu_i-\mu)(\mu_i-\mu)^T
$$
乘上每一类的样本数，是为了体现样本多的类占的权重应该更大

$S_B$称作类间散度矩阵，$S_W$称作类内散度矩阵。

优化目标依然是
$$
max_wJ(w)=\frac{w^TS_Bw}{w^TS_Ww}
$$
计算$S_w^{-1}S_B$的特征值，并且按大小排序，取前d个，并取对应的d个特征向量，为要投影到的d个超平面。

记d个特征向量为$(w_1,w_2,...,w_d)$，降维结果为
$$
x'=(w_1^Tx,w_2^Tx,...,w_d^Tx)^T
$$
有一点，因为LDA降维用到了类别信息，所以**降维最多降到k-1维，不能比k-1大**，类别数为k（这里没怎么懂，好像和矩阵的秩有关系？）

### LDA算法流程总结

1. 计算类内散度矩阵$S_W$
2. 计算类间散度矩阵$S_B$
3. 计算矩阵$S_w^{-1}S_B$的特征值，并按大小排序
4. 取前d个特征值对应的特征向量，每个数据点与这d个特征向量分别做內积，得到d维向量，即降维后的结果（或者直接说将d个特征向量构造为变换矩阵，对原始数据点变换后得到降维后的结果）

### LDA优缺点

#### 优点

1. 在降维中使用了类别的先验信息

#### 缺点

1. 不适合对非高斯分布样本进行降维（不太懂）
2. 降维降到的维数受限制，只能小于等于k-1
3. 当样本分类信息依赖方差而不是均值的时候，效果不好（因为计算类间距离是用的类均值，即用均值来代表这个类了）
4. 容易过拟合

## PCA与LDA联系、区别

### 相同点

1. 都是降维算法（2333）
2. 都使用了矩阵特征分解的思想
3. 都假设样本符合高斯分布（不太懂）

### 不同点

1. LDA有监督，PCA无监督
2. LDA最多降到类别数-1维，PCA没有限制
3. LDA还可以用于分类，PCA不行
4. LDA选择的是分类性能最好的投影方向，PCA选择的是样本投影坐标方差分布最大的投影方向

